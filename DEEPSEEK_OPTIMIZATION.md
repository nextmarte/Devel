# üöÄ Estrat√©gias de Otimiza√ß√£o - Deepseek

## üìä Problemas Identificados

1. **Execu√ß√£o Sequencial dos Flows**
   - Corre√ß√£o ‚Üí Identifica√ß√£o ‚Üí Sum√°rio (um por um)
   - Esperando cada chamada terminar antes de iniciar a pr√≥xima
   - Tempo total = soma de todos os tempos

2. **Prompts Muito Grandes**
   - Textos completos sendo enviados para cada flow
   - Deepseek cobra por tokens ‚Üí mais tokens = mais caro e lento

3. **Sem Cache/Memoiza√ß√£o**
   - Mesmos prompts s√£o reprocessados
   - Sem reutiliza√ß√£o de resultados

4. **Modelo Padr√£o Pode N√£o Ser √ìtimo**
   - Usando `deepseek-chat` para tudo
   - N√£o aproveitando `deepseek-reasoner` quando necess√°rio

## ‚úÖ Solu√ß√µes Propostas

### 1. **Executar Flows em Paralelo** ‚ö° (Impacto: Alto - 60-70% mais r√°pido)

```typescript
// ‚ùå ANTES (sequencial)
const correctedResult = await correctTranscriptionErrors({...});
const speakersResult = await identifySpeakers({...});
const summaryResult = await summarizeText({...});

// ‚úÖ DEPOIS (paralelo)
const [correctedResult, speakersResult, summaryResult] = await Promise.all([
  correctTranscriptionErrors({...}),
  identifySpeakers({...}),
  summarizeText({...})
]);
```

**Implementa√ß√£o**: Arquivo `src/app/api/jobs/[jobId]/route.ts` - fun√ß√£o `processFlowsServer`

---

### 2. **Implementar Cache em Redis** üíæ (Impacto: M√©dio - 90% mais r√°pido para repeats)

```typescript
// Exemplo de chave de cache
const cacheKey = `deepseek:${hashOfPrompt}`;
const cached = await redis.get(cacheKey);
if (cached) return cached;

const result = await generateWithDeepseek(prompt);
await redis.set(cacheKey, result, { ex: 3600 }); // 1 hora
return result;
```

**Benef√≠cios**:
- Prompts id√™nticos n√£o precisam ir ao Deepseek
- Redis na mem√≥ria (muito r√°pido)

---

### 3. **Resumir Texto Antes de Processar** üìù (Impacto: Alto - 50% menos tokens)

```typescript
// Antes: 100.000 caracteres
// Depois: Extrair apenas key sentences (~20.000 caracteres)

const summaryForProcessing = await extractKeySentences(rawTranscription);
// Usar summaryForProcessing nos flows em vez do texto completo
```

**Como fazer**:
- Usar regex para extrair frases principais
- Ou chamar Deepseek com `max_tokens: 2000` para pr√©-resumir

---

### 4. **Usar Modelo Mais R√°pido** ‚ö° (Impacto: M√©dio - 2-3x mais r√°pido)

```typescript
// Deepseek tem diferentes modelos:
// - deepseek-chat (mais r√°pido, bom custo-benef√≠cio)
// - deepseek-reasoner (mais lento, mais preciso - use s√≥ quando necess√°rio)

// Para corre√ß√£o e identifica√ß√£o: use chat
// Para sum√°rio complexo: considere reasoner
```

**Estrat√©gia**:
- Correction: `deepseek-chat` (r√°pido)
- Identify Speakers: `deepseek-chat` (r√°pido)
- Summary: Depende da complexidade - testar ambos

---

### 5. **Batch Prompts** üì¶ (Impacto: M√©dio - 30% mais r√°pido + barato)

```typescript
// ‚ùå ANTES - 3 chamadas
await generateWithDeepseek("Corrigir: " + text);
await generateWithDeepseek("Identificar speakers: " + text);
await generateWithDeepseek("Summarizar: " + text);

// ‚úÖ DEPOIS - 1 chamada
const batchPrompt = `
[TAREFA 1] Corrigir: ${text}
---
[TAREFA 2] Identificar speakers: ${text}
---
[TAREFA 3] Summarizar: ${text}
`;
const result = await generateWithDeepseek(batchPrompt);
// Parse result para extrair as 3 respostas
```

**Vantagens**:
- 1 chamada em vez de 3 = 3x mais r√°pido (overhead de rede)
- Setup de contexto √∫nico
- Mais barato

---

### 6. **Adicionar Timeout e Fallback** ‚è±Ô∏è (Impacto: Seguran√ßa)

```typescript
async function generateWithDeepseekTimeout(
  prompt: string, 
  timeoutMs: number = 30000
): Promise<string> {
  return Promise.race([
    generateWithDeepseek(prompt),
    new Promise<string>((_, reject) =>
      setTimeout(() => reject(new Error('Timeout')), timeoutMs)
    )
  ]);
}
```

---

### 7. **Truncar/Resumir Automaticamente** ‚úÇÔ∏è (Impacto: Alto - 80% menos tokens para textos grandes)

```typescript
function truncateText(text: string, maxChars: number = 5000): string {
  if (text.length <= maxChars) return text;
  
  // Encontrar √∫ltima frase completa antes do limite
  const truncated = text.substring(0, maxChars);
  const lastPeriod = truncated.lastIndexOf('.');
  return text.substring(0, lastPeriod + 1);
}

// Usar em todos os flows
const truncatedText = truncateText(input.text, 5000);
```

---

## üéØ Plano de Implementa√ß√£o (Priorizado)

### Fase 1: R√°pida (30 min) - Impacto: 60-70% mais r√°pido
- [ ] **Executar flows em paralelo** (m√°ximo impacto + f√°cil)
- Arquivo: `src/app/api/jobs/[jobId]/route.ts`
- Mudan√ßa: Usar `Promise.all()` em `processFlowsServer()`

### Fase 2: M√©dia (1-2 horas) - Impacto: 50% menos tokens
- [ ] **Truncar textos automaticamente**
- Arquivo: `src/ai/genkit.ts` + flows
- Mudan√ßa: Adicionar fun√ß√£o `truncateText()` e usar antes de chamar Deepseek

### Fase 3: Longa (2-3 horas) - Impacto: 90% para repeats
- [ ] **Implementar Redis cache**
- Arquivo: Novo `src/lib/deepseek-cache.ts`
- Depend√™ncia: Redis rodando em Docker

### Fase 4: Otimiza√ß√£o (opcional) - Impacto: 30% mais r√°pido
- [ ] **Batch prompts**
- Arquivo: Novo `src/lib/deepseek-batch.ts`
- Complexidade: M√©dia (parsing de resultados)

---

## üìà Benchmarks Esperados

| Estrat√©gia | Tempo Antes | Tempo Depois | Redu√ß√£o | Investimento |
|-----------|------------|-------------|---------|--------------|
| Paralelo | 30s | 10s | 67% | 30 min |
| Truncar | 10s | 5s | 50% | 1h |
| Redis Cache | 5s (repeat) | 0.5s | 90% | 2h |
| Batch | 10s | 7s | 30% | 2h |
| **TOTAL** | **30s** | **~5s** | **~85%** | **5-6h** |

---

## üîß C√≥digo Base para Come√ßar

### Op√ß√£o 1: Paralelo (Recomendado para come√ßar)

```typescript
// Em src/app/api/jobs/[jobId]/route.ts

async function processFlowsServer(jobId: string, rawTranscription: string, generateSummary: boolean = false) {
  try {
    console.log(`[FLOWS-SERVER] üöÄ Iniciando processamento paralelo...`);
    const startTime = Date.now();
    
    // Preparar promises
    const flowPromises = [
      correctTranscriptionErrors({
        transcription: rawTranscription,
        jobId,
      }),
      identifySpeakers({
        text: rawTranscription,
        jobId,
      })
    ];
    
    if (generateSummary) {
      flowPromises.push(
        summarizeText({
          text: rawTranscription,
          jobId,
        })
      );
    }
    
    // Executar tudo em paralelo
    const [correctedResult, speakersResult, summaryResult] = await Promise.all(flowPromises);
    
    const elapsed = Date.now() - startTime;
    console.log(`[FLOWS-SERVER] ‚úÖ Processamento paralelo completado em ${elapsed}ms`);
    
    return {
      correctedTranscription: correctedResult?.correctedTranscription || rawTranscription,
      identifiedTranscription: speakersResult?.identifiedText || rawTranscription,
      summary: summaryResult?.summary || null,
    };
  } catch (error: any) {
    console.error(`[FLOWS-SERVER] ‚ùå Erro:`, error);
    return null;
  }
}
```

### Op√ß√£o 2: Truncar Texto

```typescript
// Em src/ai/genkit.ts

export function truncateText(text: string, maxChars: number = 5000): string {
  if (text.length <= maxChars) return text;
  
  const truncated = text.substring(0, maxChars);
  const lastPeriod = truncated.lastIndexOf('.');
  return text.substring(0, lastPeriod + 1) || truncated;
}

export async function generateWithDeepseek(prompt: string): Promise<string> {
  try {
    // Truncar prompt se muito grande
    const truncatedPrompt = truncateText(prompt, 8000);
    
    const response = await deepseekClient.chat.completions.create({
      model: 'deepseek-chat',
      max_tokens: 8000,
      messages: [
        {
          role: 'user',
          content: truncatedPrompt,
        },
      ],
    });

    const firstChoice = response.choices[0];
    if (firstChoice && 'message' in firstChoice && firstChoice.message) {
      return firstChoice.message.content || '';
    }

    return '';
  } catch (error) {
    console.error('Error calling Deepseek:', error);
    throw error;
  }
}
```

---

## ‚öôÔ∏è Configura√ß√µes Recomendadas

```env
# .env.local

# Limites de texto
DEEPSEEK_MAX_PROMPT_CHARS=5000
DEEPSEEK_MAX_TOKENS=4000

# Timeout
DEEPSEEK_TIMEOUT_MS=30000

# Cache (se implementar)
REDIS_URL=redis://localhost:6379

# Modelo
DEEPSEEK_MODEL=deepseek-chat
```

---

## üìä Monitoramento

Adicionar m√©tricas nos logs:

```typescript
const startTime = Date.now();
const result = await generateWithDeepseek(prompt);
const duration = Date.now() - startTime;

console.log(`[DEEPSEEK-METRICS]`, {
  duration,
  promptLength: prompt.length,
  model: 'deepseek-chat',
  stage: 'correcting',
  timestamp: new Date().toISOString(),
});
```

---

## üéì Recursos

- [Deepseek API Docs](https://api-docs.deepseek.com/)
- [OpenAI Batch API](https://platform.openai.com/docs/guides/batch-processing)
- [Redis for Node.js](https://github.com/redis/node-redis)

---

**Status**: üìã Documento de Otimiza√ß√£o Completo - Pronto para Implementa√ß√£o
